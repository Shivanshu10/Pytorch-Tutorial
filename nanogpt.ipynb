{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81d5cd26",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-14T14:53:24.198714Z",
     "iopub.status.busy": "2025-01-14T14:53:24.198377Z",
     "iopub.status.idle": "2025-01-14T14:53:24.602143Z",
     "shell.execute_reply": "2025-01-14T14:53:24.600964Z"
    },
    "papermill": {
     "duration": 0.413224,
     "end_time": "2025-01-14T14:53:24.603909",
     "exception": false,
     "start_time": "2025-01-14T14:53:24.190685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-14 14:53:24--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 1115394 (1.1M) [text/plain]\r\n",
      "Saving to: ‘input.txt’\r\n",
      "\r\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.03s   \r\n",
      "\r\n",
      "2025-01-14 14:53:24 (30.8 MB/s) - ‘input.txt’ saved [1115394/1115394]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61ebe50d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:53:24.617654Z",
     "iopub.status.busy": "2025-01-14T14:53:24.617328Z",
     "iopub.status.idle": "2025-01-14T14:53:24.623098Z",
     "shell.execute_reply": "2025-01-14T14:53:24.622149Z"
    },
    "papermill": {
     "duration": 0.013781,
     "end_time": "2025-01-14T14:53:24.624339",
     "exception": false,
     "start_time": "2025-01-14T14:53:24.610558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc3b6be9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:53:24.637470Z",
     "iopub.status.busy": "2025-01-14T14:53:24.637214Z",
     "iopub.status.idle": "2025-01-14T14:53:24.641462Z",
     "shell.execute_reply": "2025-01-14T14:53:24.640727Z"
    },
    "papermill": {
     "duration": 0.012,
     "end_time": "2025-01-14T14:53:24.642722",
     "exception": false,
     "start_time": "2025-01-14T14:53:24.630722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters 1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters\", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b622118",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:53:24.655485Z",
     "iopub.status.busy": "2025-01-14T14:53:24.655201Z",
     "iopub.status.idle": "2025-01-14T14:53:24.672082Z",
     "shell.execute_reply": "2025-01-14T14:53:24.671216Z"
    },
    "papermill": {
     "duration": 0.02455,
     "end_time": "2025-01-14T14:53:24.673299",
     "exception": false,
     "start_time": "2025-01-14T14:53:24.648749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"\".join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d92d7869",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:53:24.686140Z",
     "iopub.status.busy": "2025-01-14T14:53:24.685934Z",
     "iopub.status.idle": "2025-01-14T14:53:24.690110Z",
     "shell.execute_reply": "2025-01-14T14:53:24.689306Z"
    },
    "papermill": {
     "duration": 0.012028,
     "end_time": "2025-01-14T14:53:24.691462",
     "exception": false,
     "start_time": "2025-01-14T14:53:24.679434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stoi = { ch: i for i,ch in enumerate(chars) }\n",
    "itos = { i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: \"\".join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8e54f95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:53:24.704692Z",
     "iopub.status.busy": "2025-01-14T14:53:24.704436Z",
     "iopub.status.idle": "2025-01-14T14:53:24.708545Z",
     "shell.execute_reply": "2025-01-14T14:53:24.707770Z"
    },
    "papermill": {
     "duration": 0.012182,
     "end_time": "2025-01-14T14:53:24.709847",
     "exception": false,
     "start_time": "2025-01-14T14:53:24.697665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "725a7bed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:53:24.723128Z",
     "iopub.status.busy": "2025-01-14T14:53:24.722881Z",
     "iopub.status.idle": "2025-01-14T14:53:27.750580Z",
     "shell.execute_reply": "2025-01-14T14:53:27.749520Z"
    },
    "papermill": {
     "duration": 3.035886,
     "end_time": "2025-01-14T14:53:27.752044",
     "exception": false,
     "start_time": "2025-01-14T14:53:24.716158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# encode entire dataset\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "201c8e77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:53:27.766062Z",
     "iopub.status.busy": "2025-01-14T14:53:27.765734Z",
     "iopub.status.idle": "2025-01-14T14:53:27.769185Z",
     "shell.execute_reply": "2025-01-14T14:53:27.768614Z"
    },
    "papermill": {
     "duration": 0.011584,
     "end_time": "2025-01-14T14:53:27.770385",
     "exception": false,
     "start_time": "2025-01-14T14:53:27.758801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "n = int(0.9*len(data)) # first 90% will be train\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f667ac55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:53:27.783837Z",
     "iopub.status.busy": "2025-01-14T14:53:27.783600Z",
     "iopub.status.idle": "2025-01-14T14:53:27.791787Z",
     "shell.execute_reply": "2025-01-14T14:53:27.790960Z"
    },
    "papermill": {
     "duration": 0.016311,
     "end_time": "2025-01-14T14:53:27.792974",
     "exception": false,
     "start_time": "2025-01-14T14:53:27.776663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target 47\n",
      "when input is tensor([18, 47]) the target 56\n",
      "when input is tensor([18, 47, 56]) the target 57\n",
      "when input is tensor([18, 47, 56, 57]) the target 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target 58\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00219598",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:53:27.806088Z",
     "iopub.status.busy": "2025-01-14T14:53:27.805852Z",
     "iopub.status.idle": "2025-01-14T14:53:27.840917Z",
     "shell.execute_reply": "2025-01-14T14:53:27.839894Z"
    },
    "papermill": {
     "duration": 0.043043,
     "end_time": "2025-01-14T14:53:27.842222",
     "exception": false,
     "start_time": "2025-01-14T14:53:27.799179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "when input is [24] the target is 43\n",
      "when input is [24, 43] the target is 58\n",
      "when input is [24, 43, 58] the target is 5\n",
      "when input is [24, 43, 58, 5] the target is 57\n",
      "when input is [24, 43, 58, 5, 57] the target is 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target is 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target is 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target is 39\n",
      "when input is [44] the target is 53\n",
      "when input is [44, 53] the target is 56\n",
      "when input is [44, 53, 56] the target is 1\n",
      "when input is [44, 53, 56, 1] the target is 58\n",
      "when input is [44, 53, 56, 1, 58] the target is 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target is 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target is 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target is 1\n",
      "when input is [52] the target is 58\n",
      "when input is [52, 58] the target is 1\n",
      "when input is [52, 58, 1] the target is 58\n",
      "when input is [52, 58, 1, 58] the target is 46\n",
      "when input is [52, 58, 1, 58, 46] the target is 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target is 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target is 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target is 46\n",
      "when input is [25] the target is 17\n",
      "when input is [25, 17] the target is 27\n",
      "when input is [25, 17, 27] the target is 10\n",
      "when input is [25, 17, 27, 10] the target is 0\n",
      "when input is [25, 17, 27, 10, 0] the target is 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target is 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target is 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target is 39\n"
     ]
    }
   ],
   "source": [
    "# create batches\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split==\"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, )) # random offsets in training set\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # for each offset get x\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch(\"train\")\n",
    "print(\"inputs:\")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"targets:\")\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print(\"----\")\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {context.tolist()} the target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c267397",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:53:27.856211Z",
     "iopub.status.busy": "2025-01-14T14:53:27.855968Z",
     "iopub.status.idle": "2025-01-14T14:53:27.941895Z",
     "shell.execute_reply": "2025-01-14T14:53:27.941053Z"
    },
    "papermill": {
     "duration": 0.093996,
     "end_time": "2025-01-14T14:53:27.943184",
     "exception": false,
     "start_time": "2025-01-14T14:53:27.849188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "# baseline model is bigram\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token simply reads logits from the lookpup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T) \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            # ocus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "bigramModel = BigramLanguageModel(vocab_size)\n",
    "logits, loss = bigramModel(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(bigramModel.generate(idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b8eb8cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:53:27.957490Z",
     "iopub.status.busy": "2025-01-14T14:53:27.957178Z",
     "iopub.status.idle": "2025-01-14T14:53:27.961562Z",
     "shell.execute_reply": "2025-01-14T14:53:27.960802Z"
    },
    "papermill": {
     "duration": 0.012701,
     "end_time": "2025-01-14T14:53:27.962825",
     "exception": false,
     "start_time": "2025-01-14T14:53:27.950124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37ca65f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:53:27.976935Z",
     "iopub.status.busy": "2025-01-14T14:53:27.976653Z",
     "iopub.status.idle": "2025-01-14T14:53:29.155047Z",
     "shell.execute_reply": "2025-01-14T14:53:29.154363Z"
    },
    "papermill": {
     "duration": 1.186956,
     "end_time": "2025-01-14T14:53:29.156528",
     "exception": false,
     "start_time": "2025-01-14T14:53:27.969572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(bigramModel.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb656368",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:53:29.171908Z",
     "iopub.status.busy": "2025-01-14T14:53:29.171497Z",
     "iopub.status.idle": "2025-01-14T14:53:45.521189Z",
     "shell.execute_reply": "2025-01-14T14:53:45.520149Z"
    },
    "papermill": {
     "duration": 16.358791,
     "end_time": "2025-01-14T14:53:45.522612",
     "exception": false,
     "start_time": "2025-01-14T14:53:29.163821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5727508068084717\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range (10000):\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    logits, loss = bigramModel(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dac972d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:53:45.536958Z",
     "iopub.status.busy": "2025-01-14T14:53:45.536715Z",
     "iopub.status.idle": "2025-01-14T14:53:45.553281Z",
     "shell.execute_reply": "2025-01-14T14:53:45.552549Z"
    },
    "papermill": {
     "duration": 0.024999,
     "end_time": "2025-01-14T14:53:45.554586",
     "exception": false,
     "start_time": "2025-01-14T14:53:45.529587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iyoteng h hasbe pave pirance\n",
      "Rie hicomyonthar's\n",
      "Plinseard ith henoure wounonthioneir thondy, y helti\n"
     ]
    }
   ],
   "source": [
    "print(decode(bigramModel.generate(idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7679bd2f",
   "metadata": {
    "papermill": {
     "duration": 0.006403,
     "end_time": "2025-01-14T14:53:45.567841",
     "exception": false,
     "start_time": "2025-01-14T14:53:45.561438",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Mathematical trick used in self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fb230c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:53:45.581582Z",
     "iopub.status.busy": "2025-01-14T14:53:45.581310Z",
     "iopub.status.idle": "2025-01-14T14:53:45.614192Z",
     "shell.execute_reply": "2025-01-14T14:53:45.613139Z"
    },
    "papermill": {
     "duration": 0.0413,
     "end_time": "2025-01-14T14:53:45.615617",
     "exception": false,
     "start_time": "2025-01-14T14:53:45.574317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "--\n",
      "b=\n",
      "tensor([[8., 6.],\n",
      "        [5., 2.],\n",
      "        [4., 4.]])\n",
      "--\n",
      "c=\n",
      "tensor([[17., 12.],\n",
      "        [17., 12.],\n",
      "        [17., 12.]])\n",
      "a=\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "--\n",
      "b=\n",
      "tensor([[8., 6.],\n",
      "        [5., 2.],\n",
      "        [4., 4.]])\n",
      "--\n",
      "c=\n",
      "tensor([[ 8.,  6.],\n",
      "        [13.,  8.],\n",
      "        [17., 12.]])\n"
     ]
    }
   ],
   "source": [
    " # example\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2 # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "# we want to couple elements in x but we have few conditions\n",
    "# information only travels from prev timestamp to current\n",
    "\n",
    "# one way to do it is by taking mean of prev timestamp elements and use it to influence current timestamp element\n",
    "# we want x[b, t] = mean_{i<=t} x[b, i]\n",
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] # (t, C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n",
    "        xbow[b, t] = torch.mean(xprev, 0)\n",
    "\n",
    "# but this is inefficient\n",
    "# matrix mul way to do it \n",
    "a = torch.ones(3, 3)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "print(\"a=\")\n",
    "print(a)\n",
    "print(\"--\")\n",
    "print(\"b=\")\n",
    "print(b)\n",
    "print(\"--\")\n",
    "print(\"c=\")\n",
    "print(c)\n",
    "# so c is simply row wise sum of column elements of b\n",
    "\n",
    "# when we set a as lower triangular matrix\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "c = a @ b\n",
    "print(\"a=\")\n",
    "print(a)\n",
    "print(\"--\")\n",
    "print(\"b=\")\n",
    "print(b)\n",
    "print(\"--\")\n",
    "print(\"c=\")\n",
    "print(c)\n",
    "# c will be col wise sum of prev elements of b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2546c208",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:53:45.630556Z",
     "iopub.status.busy": "2025-01-14T14:53:45.630283Z",
     "iopub.status.idle": "2025-01-14T14:53:45.644710Z",
     "shell.execute_reply": "2025-01-14T14:53:45.643886Z"
    },
    "papermill": {
     "duration": 0.02286,
     "end_time": "2025-01-14T14:53:45.646005",
     "exception": false,
     "start_time": "2025-01-14T14:53:45.623145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "------------------------------------------\n",
      "tensor([[[ 0.1808, -0.0700],\n",
      "         [-0.0894, -0.4926],\n",
      "         [ 0.1490, -0.3199],\n",
      "         [ 0.3504, -0.2238],\n",
      "         [ 0.3525,  0.0545],\n",
      "         [ 0.0688, -0.0396],\n",
      "         [ 0.0927, -0.0682],\n",
      "         [-0.0341,  0.1332]],\n",
      "\n",
      "        [[ 1.3488, -0.1396],\n",
      "         [ 0.8173,  0.4127],\n",
      "         [-0.1342,  0.4395],\n",
      "         [ 0.2711,  0.4774],\n",
      "         [ 0.2421,  0.0694],\n",
      "         [ 0.0084,  0.0020],\n",
      "         [ 0.0712, -0.1128],\n",
      "         [ 0.2527,  0.2149]],\n",
      "\n",
      "        [[-0.6631, -0.2513],\n",
      "         [ 0.1735, -0.0649],\n",
      "         [ 0.1685,  0.3348],\n",
      "         [-0.1621,  0.1765],\n",
      "         [-0.2312, -0.0436],\n",
      "         [-0.1015, -0.2855],\n",
      "         [-0.2593, -0.1630],\n",
      "         [-0.3015, -0.2293]],\n",
      "\n",
      "        [[ 1.6455, -0.8030],\n",
      "         [ 1.4985, -0.5395],\n",
      "         [ 0.4954,  0.3420],\n",
      "         [ 1.0623, -0.1802],\n",
      "         [ 1.1401, -0.4462],\n",
      "         [ 1.0870, -0.4071],\n",
      "         [ 1.0430, -0.1299],\n",
      "         [ 1.1138, -0.1641]]])\n",
      "------------------------------------------\n",
      "tensor([[[ 0.1808, -0.0700],\n",
      "         [-0.0894, -0.4926],\n",
      "         [ 0.1490, -0.3199],\n",
      "         [ 0.3504, -0.2238],\n",
      "         [ 0.3525,  0.0545],\n",
      "         [ 0.0688, -0.0396],\n",
      "         [ 0.0927, -0.0682],\n",
      "         [-0.0341,  0.1332]],\n",
      "\n",
      "        [[ 1.3488, -0.1396],\n",
      "         [ 0.8173,  0.4127],\n",
      "         [-0.1342,  0.4395],\n",
      "         [ 0.2711,  0.4774],\n",
      "         [ 0.2421,  0.0694],\n",
      "         [ 0.0084,  0.0020],\n",
      "         [ 0.0712, -0.1128],\n",
      "         [ 0.2527,  0.2149]],\n",
      "\n",
      "        [[-0.6631, -0.2513],\n",
      "         [ 0.1735, -0.0649],\n",
      "         [ 0.1685,  0.3348],\n",
      "         [-0.1621,  0.1765],\n",
      "         [-0.2312, -0.0436],\n",
      "         [-0.1015, -0.2855],\n",
      "         [-0.2593, -0.1630],\n",
      "         [-0.3015, -0.2293]],\n",
      "\n",
      "        [[ 1.6455, -0.8030],\n",
      "         [ 1.4985, -0.5395],\n",
      "         [ 0.4954,  0.3420],\n",
      "         [ 1.0623, -0.1802],\n",
      "         [ 1.1401, -0.4462],\n",
      "         [ 1.0870, -0.4071],\n",
      "         [ 1.0430, -0.1299],\n",
      "         [ 1.1138, -0.1641]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True) # how of each eleemnt we want to add when calc mean\n",
    "print(wei)\n",
    "print(\"------------------------------------------\")\n",
    "xbow2 = wei @ x\n",
    "print(xbow)\n",
    "print(\"------------------------------------------\")\n",
    "print(xbow2)\n",
    "torch.allclose(xbow, xbow, rtol= 1e-04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ac2ad4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:53:45.660670Z",
     "iopub.status.busy": "2025-01-14T14:53:45.660435Z",
     "iopub.status.idle": "2025-01-14T14:53:45.669961Z",
     "shell.execute_reply": "2025-01-14T14:53:45.669162Z"
    },
    "papermill": {
     "duration": 0.01821,
     "end_time": "2025-01-14T14:53:45.671182",
     "exception": false,
     "start_time": "2025-01-14T14:53:45.652972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tril\n",
      "--------------------\n",
      "wei\n",
      "--------------------\n",
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "masked wei\n",
      "--------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also do samething with softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "print(\"tril\")\n",
    "print(\"--------------------\")\n",
    "wei = torch.zeros((T, T))\n",
    "print(\"wei\")\n",
    "print(\"--------------------\")\n",
    "wei = wei.masked_fill(tril==0, float(\"-inf\")) # for all elem where tril==0, will become -inf\n",
    "print(wei)\n",
    "print(\"masked wei\")\n",
    "print(\"--------------------\")\n",
    "wei = F.softmax(wei, dim=1) # same matrix we got before because softmax is also normalizing func\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ff86452",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:53:45.686703Z",
     "iopub.status.busy": "2025-01-14T14:53:45.686442Z",
     "iopub.status.idle": "2025-01-14T14:53:45.727912Z",
     "shell.execute_reply": "2025-01-14T14:53:45.727120Z"
    },
    "papermill": {
     "duration": 0.0506,
     "end_time": "2025-01-14T14:53:45.729286",
     "exception": false,
     "start_time": "2025-01-14T14:53:45.678686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 65])\n",
      "tensor(4.3955, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "lN!BJ'kysLCMFJPKOL?DP-QWwrEoL?jLDJQOL.f'RIHD'Hdhs Yv,wxatnscMZwtEOS'palkq3ssZeAvzF-QT;eMk;x.gQSFCLgx\n"
     ]
    }
   ],
   "source": [
    "# slightly modefied baseline model is bigram\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "n_embd = 32\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token simply reads logits from the lookpup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # it gives us token embedding\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        token_embd = self.token_embedding_table(idx)\n",
    "        logits = self.lm_head(token_embd)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            # ocus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "bigramModel = BigramLanguageModel(vocab_size)\n",
    "logits, loss = bigramModel(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(bigramModel.generate(idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58e1583f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:53:45.745153Z",
     "iopub.status.busy": "2025-01-14T14:53:45.744893Z",
     "iopub.status.idle": "2025-01-14T14:53:45.809498Z",
     "shell.execute_reply": "2025-01-14T14:53:45.808470Z"
    },
    "papermill": {
     "duration": 0.073978,
     "end_time": "2025-01-14T14:53:45.810916",
     "exception": false,
     "start_time": "2025-01-14T14:53:45.736938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 65])\n",
      "tensor(4.4802, grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0,  4, 23, 57,  8, 27,  4, 20, 51, 54, 22, 29,  2,  6,  2, 45,  5, 50,\n",
      "         26,  9, 34, 64, 18, 25, 28, 63,  5, 46, 36, 15, 41, 46, 36,  8, 61,  8,\n",
      "         36, 14, 28, 50, 16, 26, 17,  8, 50, 21, 51,  2, 16, 50, 52, 61, 12, 20,\n",
      "         62, 15, 12, 56, 12, 61, 20, 20, 34, 62, 19,  4, 16, 50, 58, 51, 59, 63,\n",
      "         33, 30, 17, 30, 22, 18, 30, 57, 36, 33, 63, 45,  7, 26, 61, 31, 47, 51,\n",
      "         29, 39, 50, 33, 17, 39, 60, 43, 28, 16, 21]])\n",
      "\n",
      "SUGv&.pHBzdRO3yhCVjwy.R!yMTs-Ng3b3tB saBboP g;BxO.PNJNEMwXlDNYJpxsSFHYu3KXB.lehnsFhgRGFXhwIrlSX!?BeM\n"
     ]
    }
   ],
   "source": [
    "# encode position as well\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "n_embd = 32\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token simply reads logits from the lookpup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # it gives us token embedding\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        token_embd = self.token_embedding_table(idx)\n",
    "        pos_embd = self.position_embedding_table(torch.arange(T)) # encode position of each table\n",
    "        x = token_embd + pos_embd\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:] # selet only last 8 elements as new block size\n",
    "            logits, loss = self(idx_cond)\n",
    "            # ocus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "bigramModel = BigramLanguageModel(vocab_size)\n",
    "logits, loss = bigramModel(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(bigramModel.generate(idx, max_new_tokens=100))\n",
    "print(decode(bigramModel.generate(idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b788dbf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:53:45.827318Z",
     "iopub.status.busy": "2025-01-14T14:53:45.827036Z",
     "iopub.status.idle": "2025-01-14T14:53:45.838753Z",
     "shell.execute_reply": "2025-01-14T14:53:45.837824Z"
    },
    "papermill": {
     "duration": 0.021346,
     "end_time": "2025-01-14T14:53:45.840023",
     "exception": false,
     "start_time": "2025-01-14T14:53:45.818677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 32])\n",
      "torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "# add self attention\n",
    "# but we dont want simple avg of all the prev tokens\n",
    "# and we dont want to fix weight of all prev tokens \n",
    "# we want it to be data dependent\n",
    "# way to do this is following:\n",
    "# every token has two vectors: query, key\n",
    "# query vector: what im looking for, key: what do i contain\n",
    "# we get affenity by key and query by doing dot product and that becomes our wei\n",
    "# for attention instead of initializing it to be all zeros\n",
    "\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril==0, float(\"-inf\"))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "out = wei @ x\n",
    "\n",
    "print(out.shape)\n",
    "\n",
    "# single head of self attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False) # thing getting aggregated\n",
    "k = key(x)\n",
    "q = query(x)\n",
    "wei = q @ k.transpose(-2, -1) # only transpose last two dim not batch \n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril==0, float(\"-inf\"))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fdb84d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:53:45.855881Z",
     "iopub.status.busy": "2025-01-14T14:53:45.855642Z",
     "iopub.status.idle": "2025-01-14T14:53:45.858703Z",
     "shell.execute_reply": "2025-01-14T14:53:45.857868Z"
    },
    "papermill": {
     "duration": 0.01216,
     "end_time": "2025-01-14T14:53:45.859934",
     "exception": false,
     "start_time": "2025-01-14T14:53:45.847774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# attention is a communication mechanism\n",
    "# can be seen as directed graph looking at each other\n",
    "# and aggregating with weighted sum from all nodes that point to them\n",
    "# in encode attention block just delete the condition masked_fill part\n",
    "# so anything can talk to anything\n",
    "# decoder block has this mask \n",
    "# self attention keys, queries and values are all coming from same source x\n",
    "# cross attetnion doesnt has this, key and value come from seprate source as queries\n",
    "# scaled attention divides by sqrt(head_size) to get unit variance of wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "754c313e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:53:45.875338Z",
     "iopub.status.busy": "2025-01-14T14:53:45.875120Z",
     "iopub.status.idle": "2025-01-14T14:53:45.886252Z",
     "shell.execute_reply": "2025-01-14T14:53:45.885441Z"
    },
    "papermill": {
     "duration": 0.020311,
     "end_time": "2025-01-14T14:53:45.887644",
     "exception": false,
     "start_time": "2025-01-14T14:53:45.867333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "block_size = 8\n",
    "max_iters = 5000\n",
    "eavl_interval = 500\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 200\n",
    "n_embd = 200\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.sa_head = Head(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.sa_head(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "529f6aa4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:53:45.903296Z",
     "iopub.status.busy": "2025-01-14T14:53:45.903038Z",
     "iopub.status.idle": "2025-01-14T14:53:45.906591Z",
     "shell.execute_reply": "2025-01-14T14:53:45.905926Z"
    },
    "papermill": {
     "duration": 0.012742,
     "end_time": "2025-01-14T14:53:45.907830",
     "exception": false,
     "start_time": "2025-01-14T14:53:45.895088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "479790e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:53:45.923743Z",
     "iopub.status.busy": "2025-01-14T14:53:45.923448Z",
     "iopub.status.idle": "2025-01-14T14:54:09.974429Z",
     "shell.execute_reply": "2025-01-14T14:54:09.973441Z"
    },
    "papermill": {
     "duration": 24.060863,
     "end_time": "2025-01-14T14:54:09.976037",
     "exception": false,
     "start_time": "2025-01-14T14:53:45.915174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5636987686157227\n"
     ]
    }
   ],
   "source": [
    "for steps in range (5000):\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7bf7d69b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:54:09.992919Z",
     "iopub.status.busy": "2025-01-14T14:54:09.992639Z",
     "iopub.status.idle": "2025-01-14T14:54:11.355597Z",
     "shell.execute_reply": "2025-01-14T14:54:11.354790Z"
    },
    "papermill": {
     "duration": 1.372698,
     "end_time": "2025-01-14T14:54:11.357211",
     "exception": false,
     "start_time": "2025-01-14T14:54:09.984513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CKe quucoraft whint ghh many to van:\n",
      "om\n",
      "Syo; ghhlen torn thale phed\n",
      "meacyo ther idormurs!\n",
      "\n",
      "ALRIAND Hewntyo 'ithoushr hint ingoo my:\n",
      "A whipleraurtoow paist Valilt himast.\n",
      "TAT:\n",
      "of sme,'\n",
      "Thoret inouchrm, I tist.\n",
      "Thy I pontoe; thiee ngoon arm Linde hejeups, yoff tormn'st hooutr'd, I chow!\n",
      "Wauh ms hworouf whalive hom\n",
      "ofrearto k'T hand chyou, ird ieorvelire lavigheo utry, adalt chisn sHadmave\n",
      "Wend gavin, peceake twirm beand hicath-fdiespane in swar tyoiven lcomandos,\n",
      "Sduneon; peote,\n",
      "oun ofof man ill, nequuavenco! My,\n",
      "Bothe, anys:\n",
      "He\n",
      "BROTELY:\n",
      "A\n",
      "MENENELENOTORO:\n",
      "Y\n",
      "TOSh.\n",
      "\n",
      "Burche-miss theean davend\n",
      "Aus.\n",
      "This ipge.\n",
      "\n",
      "OUCLOLOFLIS:\n",
      "Ayo swaprt thont.\n",
      "\n",
      "Whakenjeou emeeld hay, Yoto ages;\n",
      "Ceatr egt ipr,\n",
      "Anmo mtoy.\n",
      "\n",
      "xnakel, hoftoinotr mak havigntooricheve mal grath bakn eseavaved dom wateptmeberveceaved of:\n",
      "Beiok tat hithrn wingg?\n",
      "Wh apne hm; fth haken aipimu'er foo llangtoe, st oy him my hust; bothel yanetilernes hafcoem haverime?\n",
      "\n",
      "Cis ine sge. 'ds?\n",
      "\n",
      "ABurr foout hak'd:\n",
      "\n",
      "Thest, youl noky oboug hes pip\n",
      "gishe mow blld yoro hem prito.\n",
      "\n",
      "MINENONTG ood totha ved ys teevenapby witers. O aptohathine; frea ntem fout nset on\n",
      "dsy nt thad atin evisild, love anchilll\n",
      "Alcasse ougothlofece ili, on tthe my wik.\n",
      "\n",
      "TROTOxfe cemhu biomaly youe por bey cs:\n",
      "Aun sy thor, tho ybourke'e kie ks,\n",
      "Cos whisallf woallil fewre,\n",
      "Lawh wre\n",
      "fo bmigave cthito.\n",
      "To tyede.\n",
      "\n",
      "Hen densimep:\n",
      "Domacurl's der ugee. Hsacknd ougard ifof, mbutf, sta htaty gant. Gy.\n",
      "\n",
      "Af womaty, bey nt orul asm Hon\n",
      "burs why, yor ofne, chos, theruke btrsingtreer olvit galg.\n",
      "\n",
      "RCK:\n",
      "Amyou ng wande?\n",
      "\n",
      "Ind shintead keln ttenwe\n",
      "g dolsem fy arersmigoh, yoous, sye kr'sem. Ad comar yo Evin. PRefo in sten ben ten ecthe the\n",
      "Sancacthe semp. Sote chor mastop. GoRmopy hioppean mbabavefro em I ty whin.\n",
      "TH:\n",
      "\n",
      "Smass, os yo, sugg yo sopoerds woungbe msemathe shent haversoing\n",
      "nbe ds thavede cedaifoaly O:\n",
      "A thend ait acul, fecre!\n",
      "\n",
      "PRIOLALO:\n",
      "Thetet jedeacavit ld\n",
      "Wooubllre, lyor Xin, whathbulr idesant:\n",
      "Serd ald habigot heet yoth ndos havengnef, I byou mamapaime fat gifouro teml\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(model.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2dc45819",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:54:11.376585Z",
     "iopub.status.busy": "2025-01-14T14:54:11.376279Z",
     "iopub.status.idle": "2025-01-14T14:54:11.387772Z",
     "shell.execute_reply": "2025-01-14T14:54:11.387039Z"
    },
    "papermill": {
     "duration": 0.022003,
     "end_time": "2025-01-14T14:54:11.389112",
     "exception": false,
     "start_time": "2025-01-14T14:54:11.367109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# multi head\n",
    "# applying multiple attention in parallel and concatenating results\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"multiple heads of self attention in parallel\"\n",
    "\n",
    "    def __init__(self, num_head, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_head)]) # store in  container\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.sa_heads = MultiHeadAttention(4, n_embd//4)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.sa_heads(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c954444e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:54:11.405231Z",
     "iopub.status.busy": "2025-01-14T14:54:11.404976Z",
     "iopub.status.idle": "2025-01-14T14:54:53.049392Z",
     "shell.execute_reply": "2025-01-14T14:54:53.048595Z"
    },
    "papermill": {
     "duration": 41.662849,
     "end_time": "2025-01-14T14:54:53.059729",
     "exception": false,
     "start_time": "2025-01-14T14:54:11.396880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.29201602935791\n"
     ]
    }
   ],
   "source": [
    "for steps in range (5000):\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e20c3d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:54:53.076097Z",
     "iopub.status.busy": "2025-01-14T14:54:53.075757Z",
     "iopub.status.idle": "2025-01-14T14:54:53.080706Z",
     "shell.execute_reply": "2025-01-14T14:54:53.079917Z"
    },
    "papermill": {
     "duration": 0.014394,
     "end_time": "2025-01-14T14:54:53.081873",
     "exception": false,
     "start_time": "2025-01-14T14:54:53.067479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"a simple linear layer followed by non-linearity\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af9a7ddc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:54:53.099002Z",
     "iopub.status.busy": "2025-01-14T14:54:53.098597Z",
     "iopub.status.idle": "2025-01-14T14:54:53.125513Z",
     "shell.execute_reply": "2025-01-14T14:54:53.124457Z"
    },
    "papermill": {
     "duration": 0.037703,
     "end_time": "2025-01-14T14:54:53.127389",
     "exception": false,
     "start_time": "2025-01-14T14:54:53.089686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block: communication followed by computation\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sa(x)\n",
    "        x = self.ffwd(x)\n",
    "        return x\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4)\n",
    "        )\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6860d390",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:54:53.146306Z",
     "iopub.status.busy": "2025-01-14T14:54:53.145952Z",
     "iopub.status.idle": "2025-01-14T14:56:48.755061Z",
     "shell.execute_reply": "2025-01-14T14:56:48.754142Z"
    },
    "papermill": {
     "duration": 115.627592,
     "end_time": "2025-01-14T14:56:48.764767",
     "exception": false,
     "start_time": "2025-01-14T14:54:53.137175",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8919646739959717\n"
     ]
    }
   ],
   "source": [
    "for steps in range (5000):\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca38a2f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:56:48.780909Z",
     "iopub.status.busy": "2025-01-14T14:56:48.780643Z",
     "iopub.status.idle": "2025-01-14T14:56:56.587621Z",
     "shell.execute_reply": "2025-01-14T14:56:56.586447Z"
    },
    "papermill": {
     "duration": 7.816636,
     "end_time": "2025-01-14T14:56:56.589022",
     "exception": false,
     "start_time": "2025-01-14T14:56:48.772386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hen spere hath se he me Tuto conqul\n",
      "Grave will han is be will spon\n",
      "to doing, and of your greeds my less a rland's ther, for night; than cuse,\n",
      "At dreat less not mat not umpoon my chom days,\n",
      "I mugh se?\n",
      "\n",
      "QUEEN BETHHORTUS:\n",
      "Rome the wive a\n",
      "be me thes al not by ment me Even\n",
      "on Ham, To thing Sends lesent of was pleely thy seading; beated so desepse? sooner meed.\n",
      "\n",
      "PAnd say.\n",
      "\n",
      "KING RICHARD II:\n",
      "Sint.\n",
      "\n",
      "FRIANA:\n",
      "If love no My king die stay the to am ray of have to mave neel main an that upon hoper jour leapy. Mare shame I love un of me, I gringued's; if this law that Nerve efessignite the congueo to sich gods atting am how oblute let not not is gaint Gret lege me, no put not sir, say of Here to hought, she, now dray, selve them; a shouls isenst:\n",
      "Ay all bOcking For hand to my confing\n",
      "he, good he prove stram to my how the arteed his trom think your seeving the king\n",
      "Hathir send his conse wherefore than you the nor my band,\n",
      "On or boon cothat eve inting triad's not dight trued.\n",
      "\n",
      "ETRHARD:\n",
      "ear your begear he for your have wak hath spifece: curlast bludaitnesse:\n",
      "And the we leap iten the at; demb, grience; ruted queen ve griess say he war his firs achaution?\n",
      "\n",
      "HENRY VINCENCE:\n",
      "And fairs these sServing wends, the ming, honour good, a pilly have se, hath cominder defe my call the longings\n",
      "I some to say; your lohent my that as and his day make your waith men the be mided.\n",
      "\n",
      "GRELYCUS:\n",
      "So the steak you?\n",
      "I peep their haty gods make no pod slongue a fath meent, thou confeld gent neverill him\n",
      "mEn reput signand awh have hous,\n",
      "With\n",
      "Dante a prine musined,\n",
      "But exrese'll thes, womagines\n",
      "For mond much a knews throught migh,\n",
      "These secan will pleat ding er chan mind not and shall dot be?\n",
      "Or live stress's nebeld frie! thou blrifes! so souse say, sin the planged your some; or you made willy a monderself! My king scatoram\n",
      "aswer will, un she sirt cousans, for past a sew,\n",
      "But dsed strank if to how he words such of these as Martay,\n",
      "A stirut and and shene:\n",
      "Let he by of mer;\n",
      "And man,\n",
      "For somern othe ap:\n",
      "ay, way tho\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(model.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d114c447",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:56:56.605557Z",
     "iopub.status.busy": "2025-01-14T14:56:56.605291Z",
     "iopub.status.idle": "2025-01-14T14:56:56.608384Z",
     "shell.execute_reply": "2025-01-14T14:56:56.607562Z"
    },
    "papermill": {
     "duration": 0.012705,
     "end_time": "2025-01-14T14:56:56.609730",
     "exception": false,
     "start_time": "2025-01-14T14:56:56.597025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# still will perform bad\n",
    "# cause we are creatin deep NN\n",
    "# which need resiudal connection to work fine\n",
    "# it reduces shrinking gradient problem in initial layers\n",
    "# cause addition divides gradient equally hence allowing starting layer also to learn something\n",
    "# add add norm for correct init\n",
    "# and add dropout to avoid overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "70e98a0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T14:56:56.625835Z",
     "iopub.status.busy": "2025-01-14T14:56:56.625585Z",
     "iopub.status.idle": "2025-01-14T15:50:37.564789Z",
     "shell.execute_reply": "2025-01-14T15:50:37.563817Z"
    },
    "papermill": {
     "duration": 3220.959816,
     "end_time": "2025-01-14T15:50:37.577136",
     "exception": false,
     "start_time": "2025-01-14T14:56:56.617320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.788929 M parameters\n",
      "step 0: train loss 4.2849, val loss 4.2823\n",
      "step 100: train loss 2.4734, val loss 2.4894\n",
      "step 200: train loss 2.4170, val loss 2.4459\n",
      "step 300: train loss 2.3254, val loss 2.3542\n",
      "step 400: train loss 2.1388, val loss 2.1893\n",
      "step 500: train loss 1.9951, val loss 2.0811\n",
      "step 600: train loss 1.8789, val loss 2.0029\n",
      "step 700: train loss 1.7882, val loss 1.9320\n",
      "step 800: train loss 1.7096, val loss 1.8644\n",
      "step 900: train loss 1.6489, val loss 1.8166\n",
      "step 1000: train loss 1.5963, val loss 1.7731\n",
      "step 1100: train loss 1.5565, val loss 1.7376\n",
      "step 1200: train loss 1.5200, val loss 1.7089\n",
      "step 1300: train loss 1.4897, val loss 1.6835\n",
      "step 1400: train loss 1.4621, val loss 1.6609\n",
      "step 1500: train loss 1.4370, val loss 1.6440\n",
      "step 1600: train loss 1.4168, val loss 1.6253\n",
      "step 1700: train loss 1.3959, val loss 1.6060\n",
      "step 1800: train loss 1.3750, val loss 1.5964\n",
      "step 1900: train loss 1.3582, val loss 1.5800\n",
      "step 2000: train loss 1.3455, val loss 1.5731\n",
      "step 2100: train loss 1.3264, val loss 1.5547\n",
      "step 2200: train loss 1.3193, val loss 1.5535\n",
      "step 2300: train loss 1.3023, val loss 1.5368\n",
      "step 2400: train loss 1.2904, val loss 1.5275\n",
      "step 2500: train loss 1.2805, val loss 1.5332\n",
      "step 2600: train loss 1.2676, val loss 1.5226\n",
      "step 2700: train loss 1.2570, val loss 1.5237\n",
      "step 2800: train loss 1.2505, val loss 1.5165\n",
      "step 2900: train loss 1.2401, val loss 1.5075\n",
      "step 3000: train loss 1.2283, val loss 1.4990\n",
      "step 3100: train loss 1.2165, val loss 1.4968\n",
      "step 3200: train loss 1.2122, val loss 1.5029\n",
      "step 3300: train loss 1.2033, val loss 1.5033\n",
      "step 3400: train loss 1.1941, val loss 1.4971\n",
      "step 3500: train loss 1.1850, val loss 1.4881\n",
      "step 3600: train loss 1.1783, val loss 1.4959\n",
      "step 3700: train loss 1.1707, val loss 1.4845\n",
      "step 3800: train loss 1.1657, val loss 1.4885\n",
      "step 3900: train loss 1.1563, val loss 1.4817\n",
      "step 4000: train loss 1.1514, val loss 1.4723\n",
      "step 4100: train loss 1.1400, val loss 1.4845\n",
      "step 4200: train loss 1.1348, val loss 1.4828\n",
      "step 4300: train loss 1.1253, val loss 1.4782\n",
      "step 4400: train loss 1.1200, val loss 1.4823\n",
      "step 4500: train loss 1.1135, val loss 1.4802\n",
      "step 4600: train loss 1.1053, val loss 1.4724\n",
      "step 4700: train loss 1.1024, val loss 1.4778\n",
      "step 4800: train loss 1.0932, val loss 1.4703\n",
      "step 4900: train loss 1.0854, val loss 1.4735\n",
      "step 4999: train loss 1.0765, val loss 1.4801\n",
      "\n",
      "And seat that thyself--mise all removers\n",
      "'Twere than denierable, and the hollow,\n",
      "And tuurn'd hath foolour out thy soul!\n",
      "\n",
      "BUCKINGHAM:\n",
      "Good my fligden:\n",
      "I then short, will I despise thee vext us,\n",
      "And would have I profess importure to strous there.\n",
      "\n",
      "GLOUCESTER:\n",
      "Blessed, bortheon.\n",
      "\n",
      "GLOUCESTER:\n",
      "I like the every yean in wantories!\n",
      "There upon the colours regrent flocks of their state.\n",
      "Who bark up you to a tidings and resigns?\n",
      "Now, thus I have other still ale, heart\n",
      "You enjoys, that my gift fanting husbant,\n",
      "Where vainsage men there from my sidde hath pertised\n",
      "With much false--\n",
      "Ashes desires for any walling one that time we have\n",
      "Had a practed by foul destraiteous.\n",
      "\n",
      "Second Citizen:\n",
      "Jesu\n",
      "Off this samedy, I charge me, to she lose had.\n",
      "\n",
      "MENENIUS:\n",
      "A pupport to follow,\n",
      "And your never majesty!\n",
      "\n",
      "MARCIUS:\n",
      "I achieved\n",
      "Out of wly: they are they had lark'd upon their worth.\n",
      "That is your good enough and all! the end.\n",
      "Gaunt, you well: shall't confull to make so?\n",
      "\n",
      "ABHORSON:\n",
      "I mean to keep a hastoman of these Ponditre,\n",
      "Resign burthenoun our sovereign seak-s a traitor.\n",
      "As any those peace! by head,\n",
      "It should have you exempt men's enjoy:\n",
      "But, as if it most cannot meet, be so,\n",
      "With not with trangle hantimen, a causel:\n",
      "Though to want away hard he himmerly,\n",
      "We will gerven, put the wanter of his hearts.\n",
      "Vaughand, then he'll bear him: to be confind\n",
      "We susaged a greaties, and able fires\n",
      "Decree her my heart! Vurgiving forget!\n",
      "\n",
      "ORTHUMBERLAND:\n",
      "Thou stand from heaven; my attending!\n",
      "\n",
      "CLIFFORD:\n",
      "What offenching fear! open and judge,\n",
      "We'll forth thy heart attendance: millain!\n",
      "Either, thou darleland my kingly tongue,\n",
      "I challe be foughione a feet.\n",
      "Thus well, delized our soldier, fair on love,\n",
      "Thou art a prayer, gait no mortal mon colver\n",
      "My umbroke: of 't.'\n",
      "\n",
      "First Servingman:\n",
      "The myre.\n",
      "\n",
      "Second Murderer:\n",
      "Ay, and that I kill her.\n",
      "\n",
      "Sold:\n",
      "\n",
      "LUCIO:\n",
      "Pray thee, let her faceful issue learn.\n",
      "\n",
      "LUCIO:\n",
      "hie your honour.\n",
      "\n",
      "All:\n",
      "Pray, sir, he known.\n",
      "\n",
      "First him Capulet:\n",
      "I'll be a more in atten.\n",
      "\n",
      "LUCIO:\n",
      "I am your swellip.\n",
      "\n",
      "POLUgnerat.\n",
      "\n",
      "GRUMIO:\n",
      "Ay, good sound, I'll sent upon thy friend; the re.\n",
      "\n",
      "POLIXENES:\n",
      "For my soul, there shall lie reasons!\n",
      "Can brats lusting with our tiedny eyes?\n",
      "It harlong respected membracheting knows\n",
      "With note privately that should he wretch?\n",
      "To meet me, I had rejoiced,\n",
      "Dedaughter 'timeant for his son he issuel.\n",
      "\n",
      "FLORIZESTA:\n",
      "Either of heavy sinfulnes, I\n",
      "Accept his uttingly resting to churge:\n",
      "Provost me more counsel.\n",
      "\n",
      "ELBOW:\n",
      "This is my please is the fire of his body.\n",
      "\n",
      "BUCKINGHAM:\n",
      "Go, sir, Grece between rise, the royal ready of flight:\n",
      "He like mosetime, be board follow in my deed;\n",
      "And, lords I enformed thee, but by my marjest.\n",
      "I am Grace by the least.\n",
      "\n",
      "DEREY:\n",
      "Then let my hears on mine hazard,\n",
      "Thine profitence lip, mercy hath breded\n",
      "Whom I have, thief my heaveny follows of the horm,\n",
      "And bid this breath day, palterous let that cries\n",
      "I' the this follow dialicat most I could do thee\n",
      "A child, not that dangerous achievory!\n",
      "For gre's esect in Hermione, let thy virtue\n",
      "Or very bed intelligence shadow, echasage of that king.\n",
      "Go hence; I am a no a curtry hither,\n",
      "And leave to the royal gate.\n",
      "\n",
      "LORD MERCY:\n",
      "Stay, so.\n",
      "You'll kill my displeasure and to leave me see\n",
      "Which your Cjudged. But shully you do them got on.\n",
      "\n",
      "BRUTUS:\n",
      "Held you'll ta'e the good. Come, good late me\n",
      "With wind upon such is safety action.\n",
      "\n",
      "LARTIUS:\n",
      "Why, shall I have all.\n",
      "\n",
      "LADY ANNE:\n",
      "Hark you his well.\n",
      "\n",
      "Gester:\n",
      "Not be this worthy dear, when he got your grace\n",
      "To meet thou against the kind well.\n",
      "There were that did stay trudge longues who showes,\n",
      "Look'd our friendshippers harful earth. Here's pain,\n",
      "Appart their humour toet.\n",
      "\n",
      "GLOUCESTER:\n",
      "Master, away!\n",
      "Come, let me both and all the quastion o't:\n",
      "When seen Richard?\n",
      "\n",
      "PETRUCHIO:\n",
      "O, malk's time Lords, and Lord Hastings;\n",
      "And thyself will give flow endleced: those fair,\n",
      "Though air dreadful nert conventure\n",
      "Of Tybalt, take hagg'd of the book.\n",
      "\n",
      "GRUMIO:\n",
      "By these some melarness, why most have in blest,\n",
      "Would burn plout'd with informous noble defence:\n",
      "But what contition at myself, who is but approach?\n",
      "But I, pray, Pompey, noble for your honour,\n",
      "I'll out be so. Taste have the'ere died;\n",
      "Your are wellen being in baptide, whose faire,\n",
      "By duty's as reaping. Where's then so, no royal:\n",
      "No, no good dear men corn, then. I two!\n",
      "Come, go with Richard; bring his pursuing numble;\n",
      "Disobed. Pruthoosom,\n",
      "Doing not reverend thy high throat,\n",
      "For I tenden thee wrong'd from the poor white tear.\n",
      "Which hath aith you butcher\n",
      "To take the honour in the deputy?\n",
      "\n",
      "STried with Forlorior uprights, that couple's fear\n",
      "Ful through, taste righter of fruit!\n",
      "Now, how deter temperate:\n",
      "I saw, true deal, fellows, their earth dialing;\n",
      "And these hands in thy seldires, hathing bred\n",
      "By her love; not in the meaful kind oppress,\n",
      "Where shall I clerge thee was the lank of verity.\n",
      "Ah, boy, liege, furth, and fury 'sheach;'\n",
      "So vice, but rich in jealousin thy flosses,\n",
      "Whilst thou fill'n it were in doing word: but what thou\n",
      "Lost i' there? by, so mucht it thou horse.\n",
      "\n",
      "OXFORD:\n",
      "How may Isabella if I lorge.\n",
      "\n",
      "CLIFFORD:\n",
      "What come your will?\n",
      "\n",
      "WESTMOW:\n",
      "Will yot on minister?\n",
      "\n",
      "WARWICK:\n",
      "No; make all ther thus good father\n",
      "Be his own descend, Tituor, too her; if you\n",
      "But he shall cust feel as yet.\n",
      "\n",
      "WARWICK:\n",
      "What to terms?\n",
      "\n",
      "KOLANDIO:\n",
      "Profane favours! Forbe well;\n",
      "They, let their advantage.\n",
      "\n",
      "Nurse:\n",
      "Come, if the right our troop; for be kneepen'd\n",
      "vices, dicatednession.\n",
      "\n",
      "Ray we take lost. Fedition; your father;\n",
      "More how Percise to the Capitol. Come, consequer\n",
      "this rephetd zeal: Could you romeo to thy lord,\n",
      "Cause for a spice of bardy in Bolingbroke.\n",
      "\n",
      "Servant:\n",
      "When we may my foot our dreadh?\n",
      "\n",
      "AUTOLYCUS:\n",
      "Nurse, give good cousin, to home it not\n",
      "a hulllonger; and though they have sues a loneaf esperay\n",
      "one every poison.\n",
      "\n",
      "Clown:\n",
      "Vouchses; get him, cousin, sir, Romeo, sir;\n",
      "they favour like to pure there.\n",
      "\n",
      "Shepherd:\n",
      "Well, I see her indection to blood, it time\n",
      "acceptations of them, with relisting herself:\n",
      "at this, awaken suffer action in war\n",
      "contradishen they have;\n",
      "Onced but little that is prival for their lords.\n",
      "\n",
      "MONTAGUE:\n",
      "'Tis welk, he is not from speak, and neg\n",
      "To give you tie know in strups me; never tield\n",
      "For yet lie you be refly to bed.\n",
      "Sicrave meny at bear my wife, in that grief a dog,\n",
      "To perform his is much put abnotle arms:\n",
      "I burnt nothing fled fortily\n",
      "Suple over Corioli at Harm's' son,\n",
      "And with his hard-sape person. Go, gently;\n",
      "Even with thee in Tower, Henry within fair couts,\n",
      "But what Warwick ish our kinsman's made. Whilst do\n",
      "Bredear France, what may be my titter: die, you\n",
      "Better unpleased the\n",
      "entral of Hermione and here she complexion from\n",
      "these love wealther's dhus.\n",
      "\n",
      "ESCALUS:\n",
      "Well, your honour less goes not hence.\n",
      "\n",
      "ISABELLA:\n",
      "I live.\n",
      "\n",
      "AEdile:\n",
      "I would have some lady short it touch much:\n",
      "Kinds fright up this rights for never mother,\n",
      "Edward, which might your predning herearture\n",
      "And your father promisence, late and death,\n",
      "So give you.\n",
      "\n",
      "ISABELLA:\n",
      "Doubly my word, to do not swear\n",
      "When Buckingham is the present;\n",
      "And this is noble empity.\n",
      "\n",
      "ABHORSEN:\n",
      "Nobleman, lourth--\n",
      "\n",
      "POMPHER:\n",
      "'Twas nothing my woman'?\n",
      "\n",
      "ANGELO:\n",
      "Of Romeo, cries.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Then my lordshooking hands that did the it is much,\n",
      "Yet not yet befell in the truth of thine.\n",
      "\n",
      "GLOUCESTER:\n",
      "So much on Jupile, Love and Warwick, that do me:\n",
      "But shall trunkle your highness; and madly daughters,\n",
      "It doth them, nor mother gave it it; achieven those\n",
      "you cannot go brother.\n",
      "But your lordship, ere you anjure to received\n",
      "The clourage boasted Applaguation.\n",
      "\n",
      "LORDSER:\n",
      "Your cheers is the seen's proper:\n",
      "I anown your chapitom; 'monizent the first,\n",
      "And which I complain the chease of Romans: there,\n",
      "I would have fed by uslain\n",
      "To aspe but te'ch descendition.\n",
      "\n",
      "LUCIO:\n",
      "I do not hope his time.\n",
      "\n",
      "JULIET:\n",
      "Yet well, that I is Traniol,\n",
      "Let this go while any now.\n",
      "\n",
      "JULIET:\n",
      "Let's have call at kill.\n",
      "\n",
      "LADY CAPULET:\n",
      "Their such prisoner of him such grief.\n",
      "\n",
      "FRIAR LAURENCE:\n",
      "At's mine:\n",
      "That I profit you, my liege,\n",
      "Putting me letter: my grandam do me in her vice\n",
      "Prefigure itself your great as I guess, I\n",
      "'lear see this gast. Nor my needinglect work father.\n",
      "\n",
      "GREEN:\n",
      "You that you were made too odder her:\n",
      "And your halm out of rotten you.\n",
      "\n",
      "Servingman:\n",
      "I will take your lordship.\n",
      "\n",
      "BLUCKINGHAM:\n",
      "I kill for him to your grace.\n",
      "\n",
      "HASTINGS:\n",
      "There is not labe hearts of nail? O, Caius,\n",
      "Our solemn from us; in the head is Richarged;\n",
      "The heaten Angelo on his sorrow;\n",
      "Whose, will I boots are forth.\n",
      "\n",
      "CLRIFCUS:\n",
      "Speak all thee, these stup do:\n",
      "I often fools, those too fool! and that Margary go alone,\n",
      "Lieu them home: mean to make men and contain;\n",
      "Which to horse drawn is nothing to replent.\n",
      "\n",
      "FROT:\n",
      "Deep your heart.\n",
      "\n",
      "ROMEO:\n",
      "Their goads lackly.\n",
      "\n",
      "KING RICHARD II:\n",
      "Yea, uncle, be bastard. Do yourself.\n",
      "\n",
      "Nurse:\n",
      "Well, what renowned date this senall'd words?\n",
      "There shall Bolingbroke pernt, loving-grief.\n",
      "O get in thee this socried way condition!\n",
      "That's good, flower, for thought we will\n",
      "Dismistre and fall'd me. Go, let me give me home.\n",
      "\n",
      "MARCIUS:\n",
      "Come, Caius Marcius.\n",
      "\n",
      "MARCIUS:\n",
      "How known in my heart?\n",
      "\n",
      "CORIOLANUS:\n",
      "Not fastisfiend!\n",
      "\n",
      "CORIOLANUS:\n",
      "I would, my word; any noble lies.\n",
      "Ere less your heart, the good womb my cage for suit.\n",
      "\n",
      "COMINIUS:\n",
      "O denying; but this royally taken,\n",
      "Art explrepainteous with the arm of league\n",
      "To the such crown of posteration,\n",
      "A city find gold of me! O lord possest,\n",
      "I'll tuld them entation to my friend,\n",
      "Whell disdescred you on your wronght and your favers\n",
      "To make him an your voyal word.\n",
      "\n",
      "KING EDWARD IV:\n",
      "What is mann'd, that I point him?\n",
      "\n",
      "GLOUCESTER:\n",
      "Petruchio, sweet shall we play all east,\n",
      "As were this one ayedy? and lute,\n",
      "Beseech mischange thy kindness bear to do attempt\n",
      "Enmiss, from the loss.\n",
      "\n",
      "KING RICHARD III:\n",
      "Ay, thus fond I still\n",
      "The very wholes deed died, when I love.\n",
      "Methius, hath always the crowned peaced;\n",
      "Our mother ones dish of eyes are dead;\n",
      "Orseal'd with breaks still, joundle pondex!\n",
      "Wealtholy intermand all. But one souls,\n",
      "I'll see them in her speedily; and yet till I make\n",
      "Frave stabber filling, and to themselves\n",
      "Laims over-eyes\n",
      "Of inference to helpsons!\n",
      "\n",
      "All:\n",
      "Grace any dieral approof,\n",
      "Let us alike of sharp were death: so even\n",
      "Did I not t\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4507fc95",
   "metadata": {
    "papermill": {
     "duration": 0.010053,
     "end_time": "2025-01-14T15:50:37.598133",
     "exception": false,
     "start_time": "2025-01-14T15:50:37.588080",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3436.894029,
   "end_time": "2025-01-14T15:50:38.932685",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-14T14:53:22.038656",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
